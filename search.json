[
  {
    "objectID": "tuto.html",
    "href": "tuto.html",
    "title": "Opening data",
    "section": "",
    "text": "RANDOM_SEED = 2306406\n\n\nimport pandas as pd \n\nurl = \"https://raw.githubusercontent.com/yiweiluo/GWStance/refs/heads/master/3_stance_detection/1_MTurk/full_annotations.tsv\"\ndf_raw = pd.read_csv(url, sep = \"\\t\") # Careful here, this document is a tsv (separator=\"\\t\" and not a csv (separator = \",\")\n\ndf_raw.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nMACE_pred\nav_rating\nsentence\nworker_0\nworker_1\nworker_2\nworker_3\nworker_4\nworker_5\nworker_6\nworker_7\nround\nbatch\nsent_id\ndisagree\nagree\nneutral\n\n\n\n\n0\n0\ndisagrees\n-1.000\nGlobal warming is a hoax.\ndisagrees\ndisagrees\ndisagrees\ndisagrees\ndisagrees\ndisagrees\ndisagrees\ndisagrees\n1\n0\ns0\nNaN\nNaN\nNaN\n\n\n1\n1\nneutral\n0.375\nAlarming levels of sea level rise are predicte...\nneutral\nneutral\nneutral\nagrees\nagrees\nneutral\nneutral\nagrees\n1\n0\ns1\nNaN\nNaN\nNaN\n\n\n2\n2\nneutral\n0.000\nOver the past several years, the United States...\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\n1\n0\ns2\nNaN\nNaN\nNaN\n\n\n3\n3\nagrees\n1.000\nGlobal warming is happening and it will be dan...\nagrees\nagrees\nagrees\nagrees\nagrees\nagrees\nagrees\nagrees\n1\n0\ns3\nNaN\nNaN\nNaN\n\n\n4\n4\nneutral\n0.000\nSome icebergs are cute.\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\n1\n0\ns4\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndf = df_raw.loc[:,[\"sent_id\", \"sentence\", \"MACE_pred\", \"av_rating\"]]\ndf = df.rename(columns={\"MACE_pred\" : \"label_text\"}) # Rename MACE_pred for conveniency\n\n\ndf.groupby([\"label_text\"]).size()\n\nlabel_text\nagrees       871\ndisagrees    441\nneutral      988\ndtype: int64\n\n\n\ndf[\"sentence-len\"] = df[\"sentence\"].apply(len)\ndf.hist(column = \"sentence-len\", by=\"label_text\")\ndf.groupby(\"label_text\")[\"sentence-len\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nlabel_text\n\n\n\n\n\n\n\n\n\n\n\n\nagrees\n871.0\n114.910448\n56.008241\n22.0\n77.0\n100.0\n148.0\n342.0\n\n\ndisagrees\n441.0\n98.090703\n59.093654\n22.0\n51.0\n86.0\n134.0\n325.0\n\n\nneutral\n988.0\n110.731781\n54.436786\n21.0\n72.0\n104.0\n148.0\n347.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf[\"sentence-len\"].hist()\ndf[\"sentence-len\"].describe()\n\ncount    2300.000000\nmean      109.890435\nstd        56.251317\nmin        21.000000\n25%        70.000000\n50%       100.000000\n75%       145.000000\nmax       347.000000\nName: sentence-len, dtype: float64\n\n\n\n\n\n\n\n\n\n\nPreprocessing data\n\ndef preprocess_text(text: str):\n    if not(isinstance(text, str)):\n        return pd.NA\n    return (\n        text\n        .replace(\"’\", \"'\")\n        .replace(\"``\", '\"')\n        .replace(\"''\", '\"')\n        .replace(\" ,\", \",\")\n        .replace(\" .\", \".\")\n        .replace(\" !\", \"!\")\n        .replace(\" ?\", \"?\")\n        .replace(\" :\", \":\")\n        .replace(\" 's\", \"'s\")\n    )\ndf[\"sentence-preprocessed\"] = df[\"sentence\"].apply(preprocess_text)\n\n\ndf.groupby(\"sentence-preprocessed\").size().value_counts()\n\n1     2032\n2        9\n50       5\nName: count, dtype: int64\n\n\n\ndf_no_duplicates = df.loc[~df[\"sent_id\"].str.startswith(\"s\"), :] \ndf_no_duplicates.groupby(\"sentence-preprocessed\").size().value_counts()\n\n1    2032\n2       9\nName: count, dtype: int64\n\n\n\ndf_concensus = df_no_duplicates.groupby(\"sentence-preprocessed\")[\"label_text\"].agg(concensus = lambda X : len(set(X)) == 1)\ndf_concensus[df_concensus[\"concensus\"] == False]\n\n\n\n\n\n\n\n\nconcensus\n\n\nsentence-preprocessed\n\n\n\n\n\nWe need to get rid of fossil fuel subsidies now.\nFalse\n\n\n\n\n\n\n\n\ndf_no_duplicates = df_no_duplicates.loc[\n    df_no_duplicates[\"sentence-preprocessed\"] != \"We need to get rid of fossil fuel subsidies now.\",\n    :\n]\ndf_no_duplicates = df_no_duplicates.drop_duplicates(\"sentence-preprocessed\")\n\n\nfrom Levenshtein import distance as lev_distance\n\nthreshold = 10\n\n# Dont run, it's annoyingly long\n\n# with open(\"Levenshtein-duplicates.txt\", \"w\") as file:\n#     file.write(\"\")\n\n# for i in range(len(df_no_duplicates)):\n#     for j in range(i + 1, len(df_no_duplicates)):\n#         s1 = df_no_duplicates.iloc[i][\"sentence-preprocessed\"]\n#         s2 = df_no_duplicates.iloc[j][\"sentence-preprocessed\"]\n#         d = lev_distance(s1, s2)\n#         if d &lt; threshold:\n#             with open(\"Levenshtein-duplicates.txt\", \"a\") as file:\n#                 file.write(f\"{d} : {s1} || {s2}\\n\")\n\n\nlast_duplicates = [\n    (\"There is no solid evidence of global warming.\",\"There is not solid evidence of global warming.\"),\n    (\"Balance of evidence suggests a discernible human influence on global climate.\",\"The balance of evidence suggests a discernible human influence on global climate.\"),\n    (\"The alleged “ consensus ” behind the dangers of anthropogenic global warming is not nearly as settled among climate scientists as people imagine.\",\"The alleged â consensus â behind the dangers of anthropogenic global warming is not nearly as settled among climate scientists as people imagine.\"),\n    (\"Rising global temperatures during the 19th and 20th centuries may be linked to greater plant photosynthesis.\",\"Rising global temperatures during the 19th and 20th centuries could be linked to greater plant photosynthesis.\"),\n    (\"Climate change will continue to affect all types of weather phenomena and subsequently impact increasingly urbanised areas.\",\"Climate change will continue to affect all types of weather phenomena and subsequently impact increasingly urbanized areas.\"),\n]\n\nfor (s1, s2) in last_duplicates:\n    lab_s1 = df_no_duplicates.loc[df_no_duplicates[\"sentence\"] == s1, \"label_text\"]\n    lab_s2 = df_no_duplicates.loc[df_no_duplicates[\"sentence\"] == s2, \"label_text\"]\n    if lab_s1.item() == lab_s2.item() : \n        df_no_duplicates.drop(index = lab_s2.index)\n    else: \n        df_no_duplicates.drop(index = [*lab_s1.index, *lab_s2.index])\n\n\nimport numpy as np \n# Create splits \nN = len(df_no_duplicates)\nN_train = int(N * 0.7)\nN_train_eval = int(N * 0.1)\nN_test = int(N * 0.1)\nN_final_eval = N - N_train - N_train_eval - N_test \n\nassert N_final_eval &gt; 0\n\n\nindices = df_no_duplicates.index.to_series()\nindices_train = (\n    indices\n    .sample(n = N_train, random_state=RANDOM_SEED)\n)\nindices_train_eval = (\n    indices\n    .drop(index=indices_train.index)\n    .sample(n = N_train_eval, random_state=RANDOM_SEED)\n)\nindices_test = (\n    indices\n    .drop(index=[*indices_train.index, *indices_train_eval.index])\n    .sample(n = N_train_eval, random_state=RANDOM_SEED)\n)\nindices_final_test = (\n    indices\n    .drop(index = [*indices_train, *indices_train_eval,*indices_test])\n)\n\n\ndf_split = (\n    pd.concat({\n        \"train\"         : df_no_duplicates.loc[indices_train      , :],\n        \"train_eval\"    : df_no_duplicates.loc[indices_train_eval , :],\n        \"test\"          : df_no_duplicates.loc[indices_test       , :],\n        \"final_test\"     : df_no_duplicates.loc[indices_final_test  , :],\n    })\n    .reset_index()\n    .drop(columns=[\"level_1\"])\n    .rename(columns = {\"level_0\": \"split\"})\n)\ndf_split.to_csv(\"./data/GWStance_preprocessed.csv\", index = False)\n\n\ndf_split\n\n\n\n\n\n\n\n\nsplit\nsent_id\nsentence\nlabel_text\nav_rating\nsentence-len\nsentence-preprocessed\n\n\n\n\n0\ntrain\nt4\nThere is a definite declining trend in Arctic ...\nagrees\n0.500\n54\nThere is a definite declining trend in Arctic ...\n\n\n1\ntrain\nt38\nHuman-made climate change is a real and urgent...\nagrees\n0.750\n138\nHuman-made climate change is a real and urgent...\n\n\n2\ntrain\nt8\nI can tell you that a lot of what happens â€” ...\nagrees\n0.875\n216\nI can tell you that a lot of what happens â€” ...\n\n\n3\ntrain\nt42\nIt is the third time in 40 years in which warm...\nneutral\n-0.250\n73\nIt is the third time in 40 years in which warm...\n\n\n4\ntrain\nt18\nA runaway greenhouse effect on Venus led to te...\nneutral\n0.125\n81\nA runaway greenhouse effect on Venus led to te...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2035\nfinal_test\nt17\nNo one can come to Davos anymore and not under...\nneutral\n-0.125\n68\nNo one can come to Davos anymore and not under...\n\n\n2036\nfinal_test\nt18\nThe climate emergency is the result of our eco...\nagrees\n0.500\n59\nThe climate emergency is the result of our eco...\n\n\n2037\nfinal_test\nt19\nSome glaciers melt while others grow and it ha...\nneutral\n-0.250\n81\nSome glaciers melt while others grow and it ha...\n\n\n2038\nfinal_test\nt29\nThe United States would still maintain a voice...\nneutral\n0.125\n94\nThe United States would still maintain a voice...\n\n\n2039\nfinal_test\nt31\nThe global warming scare is a massive hoax.\ndisagrees\n-1.000\n43\nThe global warming scare is a massive hoax.\n\n\n\n\n2040 rows × 7 columns\n\n\n\n\n\nLoading model\n\nfrom transformers import AutoModelForSequenceClassification\n\nlabels = list(df_split[\"label_text\"].unique())\nnum_labels = len(labels)\nid2label = {id:label for id, label in enumerate(labels)}\nlabel2id = {label:id for id, label in enumerate(labels)}\n\nMODEL_NAME = \"google-bert/bert-base-uncased\"\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, \n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id,                                        \n).to(device=\"cpu\")\n\n/opt/miniconda3/envs/encoder-tuto/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nprint(model)\n\nBertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)\n\n\n\nfrom transformers import AutoConfig\nprint(AutoConfig.from_pretrained(MODEL_NAME))\n\nBertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n\n\n\n\nTokenize data\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n\nfrom datasets import DatasetDict, Dataset\nfrom torch import Tensor\n\n# Create a dataset from the splits we created before\ngrouped_ds_split = df_split.groupby(\"split\")\ndsd = DatasetDict({\n    split :( \n        Dataset\n        .from_pandas(grouped_ds_split.get_group(split))\n        .with_format(\"torch\", device=\"cpu\")#, dtype=int)\n    )\n    for split in [\"train\", \"train_eval\", \"test\", \"final_test\"]\n})\n\ntokenizer_parameters = {\n    \"truncation\":True, \n    \"padding\":\"max_length\",\n    \"max_length\":400,\n    \"return_tensors\":\"pt\"\n}\n\ndef preprocess_dataset(row: dict):\n    tokenized_entry = tokenizer(row[\"sentence-preprocessed\"], **tokenizer_parameters)\n    id_label = int(label2id[row[\"label_text\"]])\n    id_label_as_tensor = Tensor([int(i == id_label) for i in range(num_labels)])\n    return {\n        **row.copy(),\n        \"labels\": id_label_as_tensor,\n        \"attention_mask\" : tokenized_entry[\"attention_mask\"].reshape(-1),\n        \"input_ids\" : tokenized_entry[\"input_ids\"].reshape(-1)\n    }\n\n\ndsd = dsd.map(preprocess_dataset, batch_size=32)\ndsd\n\nMap:   0%|          | 0/1428 [00:00&lt;?, ? examples/s]Map: 100%|██████████| 1428/1428 [00:00&lt;00:00, 3045.39 examples/s]\nMap: 100%|██████████| 204/204 [00:00&lt;00:00, 2958.24 examples/s]\nMap: 100%|██████████| 204/204 [00:00&lt;00:00, 3000.79 examples/s]\nMap: 100%|██████████| 204/204 [00:00&lt;00:00, 2951.38 examples/s]\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['split', 'sent_id', 'sentence', 'label_text', 'av_rating', 'sentence-len', 'sentence-preprocessed', 'labels', 'attention_mask', 'input_ids'],\n        num_rows: 1428\n    })\n    train_eval: Dataset({\n        features: ['split', 'sent_id', 'sentence', 'label_text', 'av_rating', 'sentence-len', 'sentence-preprocessed', 'labels', 'attention_mask', 'input_ids'],\n        num_rows: 204\n    })\n    test: Dataset({\n        features: ['split', 'sent_id', 'sentence', 'label_text', 'av_rating', 'sentence-len', 'sentence-preprocessed', 'labels', 'attention_mask', 'input_ids'],\n        num_rows: 204\n    })\n    final_test: Dataset({\n        features: ['split', 'sent_id', 'sentence', 'label_text', 'av_rating', 'sentence-len', 'sentence-preprocessed', 'labels', 'attention_mask', 'input_ids'],\n        num_rows: 204\n    })\n})\n\n\n\ndsd.save_to_disk(\"./outputs/dataset-dict\")\n\nSaving the dataset (1/1 shards): 100%|██████████| 1428/1428 [00:00&lt;00:00, 140772.94 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 204/204 [00:00&lt;00:00, 37063.07 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 204/204 [00:00&lt;00:00, 36695.89 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 204/204 [00:00&lt;00:00, 45372.68 examples/s]\n\n\n\nfrom datasets import load_from_disk \n\ndsd = load_from_disk(\"./outputs/dataset-dict\")\nfor split in dsd:\n    dsd[split] = dsd[split].with_format(\"torch\", device=\"cpu\")\n\n\nimport datasets\n\ndatasets.__version__\n\n'4.5.0'\n\n\n\nfor split in dsd:\n    dsd[split] = dsd[split].with_format(\"torch\", device=\"cpu\")\n\n\n\nUnderstand the ins and outs of the pipeline\n\nentry = [\n    \"Hello World\",\n    \"This is a second query\"\n]\n\ntokenizer_parameters = {\n    \"truncation\":True, \n    \"padding\":\"max_length\",\n    \"max_length\":400,\n    \"return_tensors\":\"pt\"\n}\n\nmodel_input = tokenizer(entry,**tokenizer_parameters)\nbase_model_output = model.base_model(**model_input)\nclassif_model_output = model(**model_input)\nprint(f'''\n# model input keys: {', '.join(model_input)}\nmodel input shape (pytorch tensor): {model_input[\"input_ids\"].shape}\nbase model output keys: {', '.join(base_model_output)}\nbase model output last_hidden_state shape (pytorch tensor): {base_model_output.last_hidden_state.shape}\nclassification model output key: {', '.join(classif_model_output)}\nclassification model output logits shape (pytorch tensor): {classif_model_output.logits.shape}\n''')\n\n\n\nSetup training arguments\n\nfrom transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n\n\ntraining_arguments = TrainingArguments(\n    # Hyperparameters\n    # num_train_epochs = 5,\n    num_train_epochs = 7,\n    learning_rate = 5e-5,\n    weight_decay  = 0.0,\n    warmup_ratio  = 0.0,\n    optim = \"adamw_torch_fused\",\n    # Second order hyperparameters\n    per_device_train_batch_size = 4,\n    per_device_eval_batch_size = 4,\n    gradient_accumulation_steps = 8,\n    # Pipe\n    output_dir = \"./models/training\",\n    overwrite_output_dir=True,\n    \n    logging_strategy = \"epoch\",\n    # eval_strategy = \"epoch\",\n    eval_strategy = \"steps\",\n    eval_steps = 32,\n    save_strategy = \"epoch\",\n    # load_best_model_at_end = True,\n    # save_total_limit = 5 + 1,\n\n    disable_tqdm = False,\n)\n\n\ndsd[\"train_eval\"][\"labels\"]\n\n\n\nLaunch training\n\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\nfrom torch import Tensor\nfrom torch.nn import Sigmoid\nfrom transformers import EvalPrediction\nimport numpy as np\n\ndef multi_label_metrics(results_matrix, labels : Tensor, threshold : float = 0.5\n                        ) -&gt; dict:\n    '''Taking a results matrix (batch_size x num_labels), the function (with a \n    threshold) associates labels to the results =&gt; y_pred\n    From this y_pred matrix, evaluate the f1_micro, roc_auc and accuracy metrics\n    '''\n    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n    sigmoid = Sigmoid()\n    probs = sigmoid(Tensor(results_matrix))\n    # next, use threshold to turn them into integer predictions\n    y_pred = np.zeros(probs.shape)\n    y_pred[np.where(probs &gt;= threshold)] = 1\n    # finally, compute metrics\n    y_true = labels\n    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n    accuracy = accuracy_score(y_true, y_pred)\n    # return as dictionary\n    return {'f1_micro': f1_micro_average,\n            'f1_macro': f1_macro_average,\n             'roc_auc': roc_auc,\n             'accuracy': accuracy}\n\ndef compute_metrics(model_output):\n    if isinstance(model_output.predictions,tuple):\n        results_matrix = model_output.predictions[0]\n    else:\n        results_matrix = model_output.predictions\n\n    metrics = multi_label_metrics(results_matrix=results_matrix, \n        labels=model_output.label_ids)\n    return metrics\n\n\ntrainer = Trainer(\n    model = model, \n    args = training_arguments,\n    train_dataset=dsd[\"train\"],\n    eval_dataset=dsd[\"train_eval\"],\n    compute_metrics = compute_metrics\n)\n\ntrainer.train()\n# WARNING MODEL switches to mps need to handle that\n\n\ntraining_arguments\n\n\ndsd[\"train\"][\"input_ids\"][0]\n\n\n\nPredict the labels for the full dataset\n\ndsd[\"test\"]\n\n\nlabels_true : list[int] = []\nlabels_pred : list[int] = []\n\nfor batch in dsd[\"test\"].batch(batch_size=16, drop_last_batch=False):\n    model_input = {\n        'input_ids' : batch['input_ids'],\n        'attention_mask' : batch['attention_mask']\n    }\n\n    logits : np.ndarray = model(**model_input).logits.detach().numpy()\n    \n    batch_of_true_label = [id2label[np.argmax(row).item()] for row in batch[\"labels\"]]\n    labels_true.extend(batch_of_true_label)\n\n    batch_of_pred_label = [id2label[np.argmax(row).item()] for row in logits]\n    labels_pred.extend(batch_of_pred_label)\n\n(\n    pd.DataFrame({\n        \"predict\" : labels_pred, \n        \"gold_standard\": labels_true\n    })\n    .to_csv(\"./outputs/prediction.csv\", index = False)\n)\n\n\n\nEvaluate performance\n\nfrom sklearn.metrics import classification_report\nresults = pd.read_csv(\"./outputs/prediction.csv\")\n\nprint(classification_report(y_true = results[\"gold_standard\"], y_pred = results[\"predict\"]))\n\n\n\nRead the learning curve\n\nimport json\n\nwith open(\"./models/training/trainer_state.json\", \"r\") as file:  # MODIFY !!!\n    training_state = json.load(file)\n\nloss = []\n\nfor log in training_state [\"log_history\"]:\n    step = log[\"step\"]\n    if \"loss\" in log:\n        loss += [{\"step\": step, \"loss\": log[\"loss\"], \"split\": \"train\"}]\n    elif \"eval_loss\" in log:\n        loss += [{\"step\": step, \"loss\": log[\"eval_loss\"], \"split\": \"train-eval\"}]\n    else: \n        # thweird\n        print(log)\n\nloss = pd.DataFrame(loss)\n\n\nimport plotly.express as px \n\npx.line(loss, x = \"step\", y = \"loss\", color =  \"split\") \n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "notebook.html",
    "href": "notebook.html",
    "title": "Checkpoint",
    "section": "",
    "text": "SEED = 2306406\n\n\nimport pandas as pd \n\nurl = \"https://raw.githubusercontent.com/yiweiluo/GWStance/refs/heads/master/3_stance_detection/1_MTurk/full_annotations.tsv\"\ndf_raw = pd.read_csv(url, sep = \"\\t\") # Careful here, this document is a tsv (separator=\"\\t\" and not a csv (separator = \",\")\n\ndf_raw.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nMACE_pred\nav_rating\nsentence\nworker_0\nworker_1\nworker_2\nworker_3\nworker_4\nworker_5\nworker_6\nworker_7\nround\nbatch\nsent_id\ndisagree\nagree\nneutral\n\n\n\n\n0\n0\ndisagrees\n-1.000\nGlobal warming is a hoax.\ndisagrees\ndisagrees\ndisagrees\ndisagrees\ndisagrees\ndisagrees\ndisagrees\ndisagrees\n1\n0\ns0\nNaN\nNaN\nNaN\n\n\n1\n1\nneutral\n0.375\nAlarming levels of sea level rise are predicte...\nneutral\nneutral\nneutral\nagrees\nagrees\nneutral\nneutral\nagrees\n1\n0\ns1\nNaN\nNaN\nNaN\n\n\n2\n2\nneutral\n0.000\nOver the past several years, the United States...\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\n1\n0\ns2\nNaN\nNaN\nNaN\n\n\n3\n3\nagrees\n1.000\nGlobal warming is happening and it will be dan...\nagrees\nagrees\nagrees\nagrees\nagrees\nagrees\nagrees\nagrees\n1\n0\ns3\nNaN\nNaN\nNaN\n\n\n4\n4\nneutral\n0.000\nSome icebergs are cute.\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\n1\n0\ns4\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nt = df_raw.groupby([\"sent_id\"])[\"sentence\"].agg(\n    howMany = lambda X : len(X),\n    howManyDifferent = lambda X : len(X.unique()),\n    isOneSentence =  lambda X : len(X.unique()) == 1,\n    agg= lambda X : '\\n\\n'.join(X)\n)\nt[\"Unique sentence\"] = t[\"howMany\"] == t[\"howManyDifferent\"]\nt\n\n\n\n\n\n\n\n\nhowMany\nhowManyDifferent\nisOneSentence\nagg\nUnique sentence\n\n\nsent_id\n\n\n\n\n\n\n\n\n\ns0\n50\n1\nTrue\nGlobal warming is a hoax.\\n\\nGlobal warming is...\nFalse\n\n\ns1\n50\n1\nTrue\nAlarming levels of sea level rise are predicte...\nFalse\n\n\ns2\n50\n1\nTrue\nOver the past several years, the United States...\nFalse\n\n\ns3\n50\n1\nTrue\nGlobal warming is happening and it will be dan...\nFalse\n\n\ns4\n50\n1\nTrue\nSome icebergs are cute.\\n\\nSome icebergs are c...\nFalse\n\n\nt0\n50\n50\nFalse\nWarmer-than-normal sea surface temperatures ar...\nTrue\n\n\nt1\n50\n50\nFalse\nWe will continue to rely in part on fossil fue...\nTrue\n\n\nt10\n50\n50\nFalse\nThe actual rise in sea levels measured only 1....\nTrue\n\n\nt11\n50\n50\nFalse\nClaims of global warming have been greatly exa...\nTrue\n\n\nt12\n50\n50\nFalse\nThe Intergovernmental Panel on Climate Change ...\nTrue\n\n\nt13\n50\n50\nFalse\nEvidence now leans against global warming resu...\nTrue\n\n\nt14\n50\n50\nFalse\nFlooding has been going on forever.\\n\\nThere i...\nTrue\n\n\nt15\n50\n50\nFalse\nSimply reducing emissions will not sufficientl...\nTrue\n\n\nt16\n50\n50\nFalse\nClimate deniers blame global warming on aliens...\nTrue\n\n\nt17\n50\n50\nFalse\nGlobal warming is inevitably going to be, at b...\nTrue\n\n\nt18\n50\n50\nFalse\nSome parts of the world the world, such as the...\nTrue\n\n\nt19\n50\n50\nFalse\nIf carbon dioxide emissions continue to rise b...\nTrue\n\n\nt2\n50\n50\nFalse\nThe study is one more example that you can get...\nTrue\n\n\nt20\n50\n50\nFalse\nMillions more people around the world are thre...\nTrue\n\n\nt21\n50\n50\nFalse\n45 % of the general public view perceived glob...\nTrue\n\n\nt22\n50\n50\nFalse\nTwo billion people may be displaced by rising ...\nTrue\n\n\nt23\n50\n50\nFalse\nGlobal warming is real and threatening.\\n\\nCli...\nTrue\n\n\nt24\n50\n50\nFalse\nThere is a danger that we can get used to glob...\nTrue\n\n\nt25\n50\n50\nFalse\nDoing so will be central for getting global wa...\nTrue\n\n\nt26\n50\n50\nFalse\nGlobal sea-level rise is both undeniable and c...\nTrue\n\n\nt27\n50\n50\nFalse\nGlobal warming isn’t happening.\\n\\nData on how...\nTrue\n\n\nt28\n50\n50\nFalse\nCarbon dioxide will hit another record high in...\nTrue\n\n\nt29\n50\n50\nFalse\nYou now have a global marketplace for clean en...\nTrue\n\n\nt3\n50\n50\nFalse\nFighting global warming, the stated purpose of...\nTrue\n\n\nt30\n40\n40\nFalse\nGlobal warming had ensured that the incidence ...\nTrue\n\n\nt31\n40\n40\nFalse\nSevere climate change is coming our way, and o...\nTrue\n\n\nt32\n40\n40\nFalse\nThere is a clear connection between rising glo...\nTrue\n\n\nt33\n40\n40\nFalse\nWarming temperatures in Pacific Ocean waters o...\nTrue\n\n\nt34\n40\n40\nFalse\nGlobal warming is a major problem and deservin...\nTrue\n\n\nt35\n40\n40\nFalse\nThe weeds that produce many allergens have ada...\nTrue\n\n\nt36\n40\n40\nFalse\nEurope’s financial losses related to flooding,...\nTrue\n\n\nt37\n40\n40\nFalse\nClimate scientists themselves are increasingly...\nTrue\n\n\nt38\n40\n40\nFalse\nA mini ice age could hit UK by 2030.\\n\\nGlobal...\nTrue\n\n\nt39\n40\n40\nFalse\nGlobal emissions will continue to rise at a to...\nTrue\n\n\nt4\n50\n50\nFalse\nFour in ten Americans (40%) have personally ex...\nTrue\n\n\nt40\n20\n20\nFalse\nThe Southern Hemisphere must be balancing the ...\nTrue\n\n\nt41\n20\n20\nFalse\nChristiana Figueres, executive secretary of th...\nTrue\n\n\nt42\n20\n20\nFalse\nExisting national pledges to restrict greenhou...\nTrue\n\n\nt43\n20\n20\nFalse\nSome legitimate uncertainty about the human co...\nTrue\n\n\nt44\n20\n20\nFalse\nGlobal warming could result in expensive incre...\nTrue\n\n\nt45\n10\n10\nFalse\nEnding climate change would require `entire so...\nTrue\n\n\nt46\n10\n10\nFalse\nThe United States to exceed its 2025 commitmen...\nTrue\n\n\nt47\n10\n10\nFalse\nClimate change is a cult.\\n\\nCity officials ci...\nTrue\n\n\nt48\n10\n10\nFalse\nA 2 percent variation in H2O equals any climat...\nTrue\n\n\nt49\n10\n10\nFalse\nRepublicans need to appeal to young people who...\nTrue\n\n\nt5\n50\n50\nFalse\nFossil fuels will be used for the foreseeable ...\nTrue\n\n\nt6\n50\n50\nFalse\nHumans have a role and therefore as a matter o...\nTrue\n\n\nt7\n50\n50\nFalse\nThe global warming has become a new religion.\\...\nTrue\n\n\nt8\n50\n50\nFalse\nThe global warming crowd can’t get any crazier...\nTrue\n\n\nt9\n50\n50\nFalse\nGlobal warming is mainly caused by human activ...\nTrue\n\n\n\n\n\n\n\n\ndf = df_raw.loc[\n  ~df_raw[\"sent_id\"].str.startswith(\"s\"),\n  [\"sentence\", \"MACE_pred\", \"av_rating\"]\n]\ndf = df.rename(columns={\"MACE_pred\" : \"label_text\"})\ndf\n\n\n\n\n\n\n\n\nsentence\nlabel_text\nav_rating\n\n\n\n\n5\nWarmer-than-normal sea surface temperatures ar...\nagrees\n0.625\n\n\n6\nWe will continue to rely in part on fossil fue...\nneutral\n0.000\n\n\n7\nThe actual rise in sea levels measured only 1....\nneutral\n0.000\n\n\n8\nClaims of global warming have been greatly exa...\ndisagrees\n-1.000\n\n\n9\nThe Intergovernmental Panel on Climate Change ...\nneutral\n-0.125\n\n\n...\n...\n...\n...\n\n\n2295\nAgriculture, in general, is responsible for a ...\nneutral\n0.375\n\n\n2296\nHiring a White House \"climate change czar\" wou...\nagrees\n0.750\n\n\n2297\nMore carbon is being released than stored.\nneutral\n0.250\n\n\n2298\nNew laws are needed to crack down on climate a...\ndisagrees\n-0.750\n\n\n2299\nScaring young people young people into believi...\ndisagrees\n-0.750\n\n\n\n\n2050 rows × 3 columns\n\n\n\n\ndf.groupby([\"label_text\"]).size()\n\nlabel_text\nagrees       780\ndisagrees    391\nneutral      879\ndtype: int64\n\n\n\ndf[\"sentence-len\"] = df[\"sentence\"].apply(len)\ndf[\"sentence-len\"].hist()\ndf[\"sentence-len\"].describe()\n\ncount    2050.000000\nmean      114.340488\nstd        55.721788\nmin        21.000000\n25%        72.000000\n50%       104.000000\n75%       147.000000\nmax       347.000000\nName: sentence-len, dtype: float64\n\n\n\n\n\n\n\n\n\n\ndf.hist(column=\"av_rating\", by=\"label_text\")\n\narray([[&lt;Axes: title={'center': 'agrees'}&gt;,\n        &lt;Axes: title={'center': 'disagrees'}&gt;],\n       [&lt;Axes: title={'center': 'neutral'}&gt;, &lt;Axes: &gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n\ndf.hist(column = \"sentence-len\", by=\"label_text\")\ndf.groupby(\"label_text\")[\"sentence-len\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nlabel_text\n\n\n\n\n\n\n\n\n\n\n\n\nagrees\n780.0\n118.393590\n58.178625\n22.0\n74.75\n108.0\n153.0\n342.0\n\n\ndisagrees\n391.0\n107.437340\n56.278525\n22.0\n64.00\n97.0\n143.0\n325.0\n\n\nneutral\n879.0\n113.814562\n52.914260\n21.0\n76.00\n104.0\n144.0\n347.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.sort_values(\"sentence-len\").head(2)[\"sentence\"].unique()\n\n&lt;ArrowStringArray&gt;\n['The seas have warmed.']\nLength: 1, dtype: str\n\n\n\ndf[\"sentence-len\"] = df[\"sentence\"].apply(len)\ndf[\"sentence-len\"].hist()\n\n\n\n\n\n\n\n\n\ndf.loc[df[\"sentence-len\"] &lt; 50,:].sort_values(\"sentence-len\")\n\n\n\n\n\n\n\n\nsentence\nleaning\nsentence-len\n\n\n\n\n1715\nGr \\/ .\nNeutral\n7\n\n\n153\nHoward Dean ( D-Vt . )\nLiberal\n22\n\n\n3749\nSenator James Inhofe ( R-Okla . )\nConservative\n33\n\n\n\n\n\n\n\n\ndf.loc[df[\"sentence-len\"] &gt;= 400, :]\n\n\n\n\n\n\n\n\nsentence\nleaning\nsentence-len\n\n\n\n\n217\nIn the new security environment , the United S...\nLiberal\n412\n\n\n306\nAs to this , Dr. Drexler offers this short lis...\nNeutral\n405\n\n\n319\nMembers of the Creative Class are drawn to suc...\nLiberal\n404\n\n\n449\nLow capital costs are the result of a lot of p...\nConservative\n408\n\n\n520\nThe Obama administration , together with assor...\nConservative\n434\n\n\n557\nMost local school districts must contend with ...\nConservative\n417\n\n\n706\nUnder the circumstances , President Obama migh...\nNeutral\n418\n\n\n713\nDel Vecchio 's words manifest not merely an et...\nLiberal\n409\n\n\n871\nAbstract : For years , feminists have been war...\nLiberal\n400\n\n\n1224\nWhile experts will continue to debate the exac...\nLiberal\n400\n\n\n1240\nYet despite that campaign assertion and the fa...\nConservative\n418\n\n\n1290\nHere , Associate Director ( James Tweeny ) out...\nConservative\n437\n\n\n1318\nThe Inter Press Service reported that these co...\nLiberal\n482\n\n\n1433\nIllegal aliens are causing a financial drain ;...\nConservative\n416\n\n\n1470\nThe result has been a system that , according ...\nLiberal\n415\n\n\n1479\nIn defending the latest incursion into airline...\nConservative\n412\n\n\n1540\nThe proposal also includes creating an orderly...\nLiberal\n418\n\n\n1660\nHere , Associate Director Tweeny outlines the ...\nConservative\n423\n\n\n1712\nHowever , investors became unmoored from the e...\nConservative\n450\n\n\n1741\nOur children are not learning because the pres...\nLiberal\n413\n\n\n1818\nMultinational bodies like NAFTA , CAFTA , and ...\nConservative\n424\n\n\n1822\nMarket and patient-based health care reform pr...\nLiberal\n401\n\n\n1915\nBetween credit-card usage and using store-spon...\nLiberal\n513\n\n\n2018\nIn this telling , school vouchers are taxpayer...\nLiberal\n422\n\n\n2088\nAbstract : ( ... ) a range of public policies ...\nLiberal\n404\n\n\n2144\nIn a book he published last year laying out a ...\nLiberal\n423\n\n\n2243\nThe dominance of such worldviews -- to the det...\nConservative\n416\n\n\n2415\nSex outside a monogamous , life-long relations...\nConservative\n431\n\n\n2464\nMany of the Republican Party leaders are also ...\nLiberal\n442\n\n\n2470\nThere is nothing inherently conservative about...\nConservative\n420\n\n\n2537\nBut the cost of servicing that debt could grow...\nConservative\n404\n\n\n2676\nWithout the protection of the natural forest t...\nLiberal\n402\n\n\n2761\nSince the collapse of the subprime mortgage ma...\nLiberal\n430\n\n\n2837\nThe U.S.-led occupation privatized large porti...\nLiberal\n441\n\n\n2852\nWhile the Massachusetts Attorney General 's Of...\nLiberal\n412\n\n\n2866\nWhile `` merit pay '' is a code word for evalu...\nLiberal\n429\n\n\n2895\nMost startling , Scalia observed that in an ``...\nLiberal\n402\n\n\n2991\nRather than cultivating a moral society and in...\nConservative\n404\n\n\n3086\nAll of this -- skyrocketing costs , dumbed-dow...\nConservative\n416\n\n\n3119\nAbstract : The performance of state government...\nConservative\n410\n\n\n3139\nThat movement 's home base is Seattle 's Disco...\nLiberal\n447\n\n\n3148\nIn its eagerness to promote the teaching of cr...\nLiberal\n407\n\n\n3594\nHe suggested decentralizing government to sett...\nConservative\n422\n\n\n3639\nEchoing the recently released report from the ...\nLiberal\n405\n\n\n3651\nIn reality , the national debt , when all the ...\nConservative\n414\n\n\n3739\nEliminating subsidies and legal privileges for...\nConservative\n415\n\n\n3752\nUniversities always emphasize the importance o...\nLiberal\n402\n\n\n3830\nThe proposal also includes creating an orderly...\nLiberal\n418\n\n\n4099\nThe financial crisis through which we are stil...\nLiberal\n404\n\n\n4117\nAnd , in trying to enlist the courts in their ...\nLiberal\n400\n\n\n4144\nIn May 1933 , U.S. Senate Banking Committee co...\nLiberal\n410\n\n\n\n\n\n\n\n\ndf.hist(\"sentence-len\", by=\"leaning\")\n\narray([[&lt;Axes: title={'center': 'Conservative'}&gt;,\n        &lt;Axes: title={'center': 'Liberal'}&gt;],\n       [&lt;Axes: title={'center': 'Neutral'}&gt;, &lt;Axes: &gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n\ndf = df.loc[df[\"sentence-len\"] &gt; 50,:]\ndf[\"ID\"] = [f\"ID-{i:04}\" for i in range(len(df))]\ndf = df.set_index(\"ID\")\ndf\n\n\n\n\n\n\n\n\nsentence\nleaning\nsentence-len\n\n\nID\n\n\n\n\n\n\n\nID-0000\nUnion shop proponents point out that the `` fr...\nConservative\n228\n\n\nID-0001\nI started this section talking about how good ...\nConservative\n92\n\n\nID-0002\nHe would end foreign aid and abolish five Cabi...\nConservative\n381\n\n\nID-0003\nHere 's how the scheme works : miss the requir...\nConservative\n181\n\n\nID-0004\nAs President , Barack Obama will create a pris...\nLiberal\n309\n\n\n...\n...\n...\n...\n\n\nID-4318\nOur plan was straightforward : Make private-se...\nLiberal\n370\n\n\nID-4319\nOnly the Western occupiers have the planes to ...\nLiberal\n295\n\n\nID-4320\nWhat 's more , virtually every subsidiary Repu...\nLiberal\n258\n\n\nID-4321\nSometimes they even dole out for scientific ex...\nLiberal\n222\n\n\nID-4322\nAnother aspect of American exceptionalism we a...\nConservative\n162\n\n\n\n\n4323 rows × 3 columns\n\n\n\n\ndef preprocess_text(text: str):\n    if not(isinstance(text, str)):\n        return pd.NA\n    return (\n        text\n        .replace(\"``\", '\"')\n        .replace(\"''\", '\"')\n        .replace(\" ,\", \",\")\n        .replace(\" .\", \".\")\n        .replace(\" !\", \"!\")\n        .replace(\" ?\", \"?\")\n        .replace(\" :\", \":\")\n        .replace(\" 's\", \"'s\")\n    )\ndf[\"sentence-preprocessed\"] = df[\"sentence\"].apply(preprocess_text)\n\n\ndf\n\n\n\n\n\n\n\n\nsentence\nleaning\nsentence-len\nsentence-preprocessed\n\n\nID\n\n\n\n\n\n\n\n\nID-0000\nUnion shop proponents point out that the `` fr...\nConservative\n228\nUnion shop proponents point out that the \" fre...\n\n\nID-0001\nI started this section talking about how good ...\nConservative\n92\nI started this section talking about how good ...\n\n\nID-0002\nHe would end foreign aid and abolish five Cabi...\nConservative\n381\nHe would end foreign aid and abolish five Cabi...\n\n\nID-0003\nHere 's how the scheme works : miss the requir...\nConservative\n181\nHere's how the scheme works: miss the required...\n\n\nID-0004\nAs President , Barack Obama will create a pris...\nLiberal\n309\nAs President, Barack Obama will create a priso...\n\n\n...\n...\n...\n...\n...\n\n\nID-4318\nOur plan was straightforward : Make private-se...\nLiberal\n370\nOur plan was straightforward: Make private-sec...\n\n\nID-4319\nOnly the Western occupiers have the planes to ...\nLiberal\n295\nOnly the Western occupiers have the planes to ...\n\n\nID-4320\nWhat 's more , virtually every subsidiary Repu...\nLiberal\n258\nWhat's more, virtually every subsidiary Republ...\n\n\nID-4321\nSometimes they even dole out for scientific ex...\nLiberal\n222\nSometimes they even dole out for scientific ex...\n\n\nID-4322\nAnother aspect of American exceptionalism we a...\nConservative\n162\nAnother aspect of American exceptionalism we a...\n\n\n\n\n4323 rows × 4 columns\n\n\n\n\ndsd = ds.train_test_split(test_size = 0.2, shuffle=True, seed=SEED)\n\n# Create an eval dataset\ntemp = dsd[\"train\"].train_test_split()\ndsd[\"train\"] = temp[\"train\"]\ndsd[\"eval\"] = temp[\"test\"]\n\ndsd\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'labels_text', 'ID'],\n        num_rows: 2593\n    })\n    test: Dataset({\n        features: ['text', 'labels_text', 'ID'],\n        num_rows: 865\n    })\n    eval: Dataset({\n        features: ['text', 'labels_text', 'ID'],\n        num_rows: 865\n    })\n})\n\n\n\nlabels = list(df[\"leaning\"].unique())\nnum_labels = len(labels)\nid2label = {id:label for id, label in enumerate(labels)}\nlabel2id = {label:id for id, label in enumerate(labels)}\n\nprint(f'''\nnum_labels : {num_labels}\nid2label : {id2label}\nlabel2id : {label2id}\n'''\n)\n\n\nnum_labels : 3\nid2label : {0: 'Conservative', 1: 'Liberal', 2: 'Neutral'}\nlabel2id : {'Conservative': 0, 'Liberal': 1, 'Neutral': 2}\n\n\n\n\nfrom transformers import AutoModel, AutoModelForSequenceClassification\n\n# MODEL_NAME = \"distilbert/distilbert-base-uncased\"\nMODEL_NAME = \"google-bert/bert-base-uncased\"\nclassif_model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, \n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id                                            \n)\nclassif_model\n\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)\n\n\n\nclassif_model.config\n\nBertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"Conservative\",\n    \"1\": \"Liberal\",\n    \"2\": \"Neutral\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"Conservative\": 0,\n    \"Liberal\": 1,\n    \"Neutral\": 2\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n\nentry = [\n    \"Hello World\",\n    \"This is a second query\"\n]\n\ntokenizer_parameters = {\n    \"truncation\":True, \n    \"padding\":\"max_length\",\n    \"max_length\":400,\n    \"return_tensors\":\"pt\"\n}\n\nmodel_input = tokenizer(entry,**tokenizer_parameters)\nbase_model_output = classif_model.base_model(**model_input)\nclassif_model_output = classif_model(**model_input)\nprint(f'''\n# model input keys: {', '.join(model_input)}\nmodel input shape (pytorch tensor): {model_input[\"input_ids\"].shape}\nbase model output keys: {', '.join(base_model_output)}\nbase model output last_hidden_state shape (pytorch tensor): {base_model_output.last_hidden_state.shape}\nclassification model output key: {', '.join(classif_model_output)}\nclassification model output logits shape (pytorch tensor): {classif_model_output.logits.shape}\n''')\n\n\n# model input keys: input_ids, token_type_ids, attention_mask\nmodel input shape (pytorch tensor): torch.Size([2, 400])\nbase model output keys: last_hidden_state, pooler_output\nbase model output last_hidden_state shape (pytorch tensor): torch.Size([2, 400, 768])\nclassification model output key: logits\nclassification model output logits shape (pytorch tensor): torch.Size([2, 3])\n\n\n\n\nfrom typing import Any\n\n# TODO: make device choice better\n\ndef preprocess_dataset(row: dict[str:Any]):\n    tokenized_entry = tokenizer(row[\"text\"], **tokenizer_parameters)\n    return {\n        **row.copy(),\n        \"labels\": int(label2id[row[\"labels_text\"]]),\n        \"attention_mask\" : tokenized_entry[\"attention_mask\"].reshape(-1).to(device = \"mps\"),\n        \"input_ids\" : tokenized_entry[\"input_ids\"].reshape(-1).to(device=\"mps\")\n    }\n\ndsd = dsd.map(preprocess_dataset, batch_size=32)\n\nMap:   0%|          | 0/2593 [00:00&lt;?, ? examples/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nMap: 100%|██████████| 2593/2593 [00:04&lt;00:00, 642.75 examples/s] \nMap: 100%|██████████| 865/865 [00:01&lt;00:00, 642.04 examples/s] \nMap: 100%|██████████| 865/865 [00:01&lt;00:00, 668.04 examples/s] \n\n\n\nfrom transformers import TrainingArguments, Trainer, DataCollatorWithPadding\ntraining_arguments = TrainingArguments(\n    # Hyperparameters\n    num_train_epochs = 5,\n    learning_rate = 5e-5,\n    weight_decay  = 0.0,\n    warmup_ratio  = 0.0,\n    optim = \"adamw_torch_fused\",\n    # Second order hyperparameters\n    per_device_train_batch_size = 4,\n    per_device_eval_batch_size = 4,\n    gradient_accumulation_steps = 8,\n    # Metrics\n    # metric_for_best_model=\"f1_macro\",\n    # Pipe\n    output_dir = \"./models/training\",\n    overwrite_output_dir=True,\n    eval_strategy = \"epoch\",\n    logging_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    load_best_model_at_end = True,\n    save_total_limit = 5 + 1,\n\n    disable_tqdm = False,\n)\n\n# TODO: Use dataloader instead\ntrainer = Trainer(\n    model = classif_model, \n    args = training_arguments,\n    train_dataset=dsd[\"train\"],\n    eval_dataset=dsd[\"eval\"],\n    # data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n)\n\n\ntrainer.train()\n\n/opt/miniconda3/envs/encoder-tuto/lib/python3.11/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n  super().__init__(loader)\n\n\n\n    \n      \n      \n      [  3/410 00:07 &lt; 52:37, 0.13 it/s, Epoch 0.02/5]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 trainer.train()\n\nFile /opt/miniconda3/envs/encoder-tuto/lib/python3.11/site-packages/transformers/trainer.py:2240, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n   2238         hf_hub_utils.enable_progress_bars()\n   2239 else:\n-&gt; 2240     return inner_training_loop(\n   2241         args=args,\n   2242         resume_from_checkpoint=resume_from_checkpoint,\n   2243         trial=trial,\n   2244         ignore_keys_for_eval=ignore_keys_for_eval,\n   2245     )\n\nFile /opt/miniconda3/envs/encoder-tuto/lib/python3.11/site-packages/transformers/trainer.py:2560, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n   2554 with context():\n   2555     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n   2557 if (\n   2558     args.logging_nan_inf_filter\n   2559     and not is_torch_xla_available()\n-&gt; 2560     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n   2561 ):\n   2562     # if loss is nan or inf simply add the average of previous logged losses\n   2563     tr_loss = tr_loss + tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n   2564 else:\n\nKeyboardInterrupt: \n\n\n\n\nfrom torch.cuda import is_available as cuda_available\nfrom torch.mps import is_available as mps_available\n\nprint(\"CUDA: \", cuda_available())\nprint(\"MPS: \", mps_available())\n\nFalse\nTrue"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "",
    "text": "It is now common practice to use LLMs (encoders or decoders) to annotate texts in the context of social science research (Gilardi et al., 2023; Bonikowski et al, 2022; Do et al., 2022). This approach does not replace the theoretical work and manual annotation, as one must define the labels and what they represent, as well as annotate some elements that will be used to evaluate the LLMs’ ability to perform the classification task. On the other hand, LLMs provide cost-effective and rapid alternatives for scaling studies.\nThere are several strategies to use LLMs to annotate texts:\n\nUsing encoder models: from texts, models create embeddings (an array of several hundred of values) on which we perform the classification. This strategy is illustrated in the Augmented Social Scientist tutorial, which requires some coding skills as well as a computer capable of loading and running the model.\nUsing decoder models: from a prompt, ie the concatenation of the text to annotate as well as the codebook to do so, we ask a model to generate the labels as text. This strategy is easier to implement as fewer coding skills are required, and models run on external machines. This nonetheless has drawbacks, which we mention at the end of this page.\n\nIn a previous tutorial, we demonstrated how to make API calls with the openailibrary (available here). In this tutorial, we explore the other solution: training an encoder model. Alternatively, you can use ActiveTigger, a software developed by our team that facilitates the use of models in the social sciences.\n\n\n\nIn this tutorial we will present an overview of the techniques used to this day and experiment on a dataset of journal articles created by Luo et al. (2020). What you will learn:\n\nUnderstand the general pipeline for text classification with encoder models\nDevelop some familiarity with preprocessing and training techniques\nEvaluate models’ performance and their impact on downstream tasks\nDevelop good practices for your research\n\nWe have also included a section discussing how to train models on GPUs.\n\n\n\nFor this tutorial we use Python version 3.12 and setup the environment with the following command:\npip install -qU pandas \"transformers==4.52.4\" datasets ipykernel matplotlib torch \"accelerate&gt;=0.26.0\" scikit-learn plotly \"nbformat&gt;=4.2.0\"\nHowever, you should know that, however convenient PyTorch, accelerate, and transformers are, these libraries may be unstable depending on your computer and environment. You will face issues related to your version. We recommand using conda1 (or the virtual environments manager of your choice) to correctly setup your environment and create a requirement file of your own once you’ve reached a stable workspace. Don’t hesitate downgrading your libraries as they are likely to be more stable."
  },
  {
    "objectID": "index.html#objectives-and-materials",
    "href": "index.html#objectives-and-materials",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "",
    "text": "In this tutorial we will present an overview of the techniques used to this day and experiment on a dataset of journal articles created by Luo et al. (2020). What you will learn:\n\nUnderstand the general pipeline for text classification with encoder models\nDevelop some familiarity with preprocessing and training techniques\nEvaluate models’ performance and their impact on downstream tasks\nDevelop good practices for your research\n\nWe have also included a section discussing how to train models on GPUs."
  },
  {
    "objectID": "index.html#install-environment",
    "href": "index.html#install-environment",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "",
    "text": "For this tutorial we use Python version 3.12 and setup the environment with the following command:\npip install -qU pandas \"transformers==4.52.4\" datasets ipykernel matplotlib torch \"accelerate&gt;=0.26.0\" scikit-learn plotly \"nbformat&gt;=4.2.0\"\nHowever, you should know that, however convenient PyTorch, accelerate, and transformers are, these libraries may be unstable depending on your computer and environment. You will face issues related to your version. We recommand using conda1 (or the virtual environments manager of your choice) to correctly setup your environment and create a requirement file of your own once you’ve reached a stable workspace. Don’t hesitate downgrading your libraries as they are likely to be more stable."
  },
  {
    "objectID": "index.html#load-your-data",
    "href": "index.html#load-your-data",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "Load your data",
    "text": "Load your data\nData is available on the project’s GitHub repository, we can download it from there using Pandas.\nimport pandas as pd \n\nurl = \"https://raw.githubusercontent.com/yiweiluo/GWStance/refs/heads/master/3_stance_detection/1_MTurk/full_annotations.tsv\"\ndf_raw = pd.read_csv(url, sep = \"\\t\") # Careful here, this document is a tsv (separator=\"\\t\" and not a csv (separator = \",\")\n\ndf_raw.head()\nWe will first need to explore the dataset, get a graps of it’s content as well as possible biases.\nAs a first step, let’s make a list of all the columns:\n\nsent_id: the sentence id. It is not unique and represents something else, we will need to create a different ID.\nsentence: the sentence to annotate.\nworker_#N \\(N\\in [1,7]\\): The team tasked MTurk workers to label the data for them. Each column concatenates the labels for a given worker.\nMACE_pred: this is the final prediction. The team used the MACE framework to create a debiased label from the workers’ labels.\nav_rating: The mean of all workers’ annotations for a given sentence (with disagree = 1, neutral = 0, agree = 1).\n\nOther columns include disagree, agree and neutral but they are empty. The round and batch columns are irrelevant for our experiment.\ndf = df_raw.loc[:,[\"sent_id\", \"sentence\", \"MACE_pred\", \"av_rating\"]]\ndf = df.rename(columns={\"MACE_pred\" : \"labe_text\"}) # Rename MACE_pred for conveniency\ndf\nLet’s have a look at the number of elements per class:\ndf_raw.groupby([\"label_text\"]).size()\nMACE_pred\nagrees       871\ndisagrees    441\nneutral      988\ndtype: int64\nWe can see that there tends to be more “neutral” or “agrees” sentences. This means that “disagrees” sentences might be trickier to spot because there will be less elements to train on. We can see that e\ndf.hist(column = \"sentence-len\", by=\"label_text\")\ndf.groupby(\"label_text\")[\"sentence-len\"].describe()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlabel_text\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nagrees\n871\n114.91\n56.0082\n22\n77\n100\n148\n342\n\n\ndisagrees\n441\n98.0907\n59.0937\n22\n51\n86\n134\n325\n\n\nneutral\n988\n110.732\n54.4368\n21\n72\n104\n148\n347\n\n\n\n\n\n\nLength of text per label\n\n\nthere does not seem to be a bias regarding the length of the documents"
  },
  {
    "objectID": "index.html#preprocess-your-data",
    "href": "index.html#preprocess-your-data",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "Preprocess your data",
    "text": "Preprocess your data\n\nCheck content integrity\nThe preprocessing step is one of the most important step in the NLP pipeline, one needs to know what the corpus contains. We list a number of aspects you want to keep in mind when pre-processing your data:\n\nThe length of your documents: Depending on the model used, the context window (ie the maximum length of the documents) will vary a lot (from 500 tokens to more than 8k). Also you will need to answer this question: where is the information that you’re looking for. For some tasks, you might want to work at the sentence level because you want to find specific elements of your corpus. For other tasks, working at the document or paragraph level is acceptable because you want to assess a global quantity. Depending on your task and the model chosen, you might want to split your documents. We also want to make sure that elements have roughly the same size. Indeed, we can’t really compare a sentence of 5 words to 10 paragraphs. This needs to be conceptualised.\n\nLet’s analyse the len of sentences in order to estimate how large will the context window be:\ndf[\"sentence-len\"] = df[\"sentence\"].apply(len)\ndf[\"sentence-len\"].hist()\ndf[\"sentence-len\"].describe()\ncount    2300.000000\nmean      109.890435\nstd        56.251317\nmin        21.000000\n25%        70.000000\n50%       100.000000\n75%       145.000000\nmax       347.000000\nName: sentence-len, dtype: float64\n\n\n\n\n\n\nFigure 3: Sentence length\n\n\n\nIn the context of our experiment, the sentences are quite short and their size is quite standard.\n\nAre there any unwanted characters: If you are used to TF-IDF techniques, you might be accoustumed to filtering punctuations and stop words. This is not something that we want to do when using transformers models. Indeed these models are trained with these characters. On top of that they convey meaningful context to the model. However, you might want to filter out elements that, for your study, do not carry semantic value. For instance, when working on social media content, depending on your problematic, you will want to either keep or remove emojis for instance2. At the end of the day, you want to make sure that the elements you keep in your text carry semantic information necessary for the annotation process.\n\nOur corpus is relatively clean given that the preprocessing stage happened in the earlier stages of the study. Skimming through the text entries, we can still find some irregularities regarding the punctuation. Let’s fix this:\ndef preprocess_text(text: str):\n    if not(isinstance(text, str)):\n        return pd.NA\n    return (\n        text\n        .replace(\"``\", '\"')\n        .replace(\"''\", '\"')\n        .replace(\" ,\", \",\")\n        .replace(\" .\", \".\")\n        .replace(\" !\", \"!\")\n        .replace(\" ?\", \"?\")\n        .replace(\" :\", \":\")\n        .replace(\" 's\", \"'s\")\n    )\n\ndf[\"sentence-preprocessed\"] = df[\"sentence\"].apply(preprocess_text)\n\nAre there any duplicates? This can be a fatal flaw as training a model with repeated elements can outweight other annotation or completely confuse it.\n\nLet’s see if we have duplicates elements.\ndf.groupby(\"sentence\").size().value_counts()\n1     2034\n2        8\n50       5\nName: count, dtype: int64\nWe can see that 5 elements are duplicated 50 times. Further inverstigations let us identify that sentences with indexes starting with an “s” (['s0', 's1', 's2', 's3', 's4']) are duplicated 50 times. Let’s remove these rows.\ndf_no_duplicates = df.loc[~df[\"sent_id\"].str.startswith(\"s\"), :] \ndf_no_duplicates.groupby(\"sentence\").size().value_counts()\nWe still need to deal with the 8 elements that have a duplicate. We would like to use the drop_duplicates function, but to do so, we need to make sure that the label of the two duplicates are the same. Let’s check if some sentence are the same but there is no concensus on the label:\ndf_concensus = df_no_duplicates.groupby(\"sentence\")[\"label_text\"].agg(concensus = lambda X : len(set(X)) == 1)\nprint(df_concensus[df_concensus[\"concensus\"] == False].to_markdown())\n\n\n\nsentence\nconcensus\n\n\n\n\nWe need to get rid of fossil fuel subsidies now.\nFalse\n\n\n\nOne sentence does not reach a concensus, We are going to remove it by hand and then use the drop_duplicates function:\ndf_no_duplicates = df_no_duplicates.loc[\n    df_no_duplicates[\"sentence\"] != \"We need to get rid of fossil fuel subsidies now.\",\n    :\n]\ndf_no_duplicates = df_no_duplicates.drop_duplicates(\"sentence\")\nAs a final check, we can use the Levenshtein distance to make sure that there are no lasting duplicates:\nfrom Levenshtein import distance as lev_distance\n\nthreshold = 10\n\nfor i in range(len(df_no_duplicates)):\n    for j in range(i + 1, len(df_no_duplicates)):\n        s1 = df_no_duplicates.iloc[i][\"sentence\"]\n        s2 = df_no_duplicates.iloc[j][\"sentence\"]\n        d = lev_distance(s1, s2)\n        if d &lt; threshold:\n            print(f\"{d} : {s1} || {s2}\")\nWith this loop, we have found 6 new sentences with duplicates:\n\nGlobal warming isn’t happening. || Global warming isn’t happening.\nThere is no solid evidence of global warming. || There is not solid evidence of global warming.\nBalance of evidence suggests a discernible human influence on global climate. || The balance of evidence suggests a discernible human influence on global climate.\nThe alleged “ consensus ” behind the dangers of anthropogenic global warming is not nearly as settled among climate scientists as people imagine. || The alleged â consensus â behind the dangers of anthropogenic global warming is not nearly as settled among climate scientists as people imagine.\nRising global temperatures during the 19th and 20th centuries may be linked to greater plant photosynthesis. || Rising global temperatures during the 19th and 20th centuries could be linked to greater plant photosynthesis.\nClimate change will continue to affect all types of weather phenomena and subsequently impact increasingly urbanised areas. || Climate change will continue to affect all types of weather phenomena and subsequently impact increasingly urbanized areas.\n\nFrom there, one can choose to remove them or not. For this tutorial we will remove one of them if the two have the same label and remove both if they don’t.\nlast_duplicates = [\n    (\"Global warming isn’t happening.\",\"Global warming isn't happening.\"),\n    (\"There is no solid evidence of global warming.\",\"There is not solid evidence of global warming.\"),\n    (\"Balance of evidence suggests a discernible human influence on global climate.\",\"The balance of evidence suggests a discernible human influence on global climate.\"),\n    (\"The alleged “ consensus ” behind the dangers of anthropogenic global warming is not nearly as settled among climate scientists as people imagine.\",\"The alleged â consensus â behind the dangers of anthropogenic global warming is not nearly as settled among climate scientists as people imagine.\"),\n    (\"Rising global temperatures during the 19th and 20th centuries may be linked to greater plant photosynthesis.\",\"Rising global temperatures during the 19th and 20th centuries could be linked to greater plant photosynthesis.\"),\n    (\"Climate change will continue to affect all types of weather phenomena and subsequently impact increasingly urbanised areas.\",\"Climate change will continue to affect all types of weather phenomena and subsequently impact increasingly urbanized areas.\"),\n]\n\nfor (s1, s2) in last_duplicates:\n    lab_s1 = df_no_duplicates.loc[df_no_duplicates[\"sentence\"] == s1, \"label_text\"]\n    lab_s2 = df_no_duplicates.loc[df_no_duplicates[\"sentence\"] == s2, \"label_text\"]\n    if lab_s1.item() == lab_s2.item() : \n        df_no_duplicates.drop(index = lab_s2.index)\n    else: \n        df_no_duplicates.drop(index = [*lab_s1.index, *lab_s2.index])\n\n\nCreate splits\nThe final step is to create a train set, a train-eval set, test set and final-eval set (also called splits).\n\nthe train set is the set of sentences the model will be training and actually trained on.\nthe train-eval set is the set of sentences the model is evluated on between epochs. These sentences are “seen” by the model so using this set for final evaluation will produce boosted results.\nthe test set is the set of sentences that are used to evaluate a model and choose Hyperparameters. They are not seen by the model but they are part of the optimisation problem.\nthe final-eval set is the set of sentences that are used to produce realistic performance evaluation. this set of sentences is not seen by the model during training and prevent from biasing the hyperparameter optimisation.\n\nIn general, we allocate 70% of the data to the train set, then 10% to train eval, test set and final eval set.Depending on the size of your dataset, you might want to tweak this distribution. Ultimately, you want enough data for training and evaluation for scores to be significative.\nStratification of the splits  You dataset may contain additional variation through years, or sources. To make sure that these variation do not bias your model, you may want to stratify your splits according to the said dimensions. If you have an unbalanced dataset, you can also choose to stratify your sets in order to create balanced splits.\nIn our case, we don’t have additional metadata to stratify our splits, and classes but the classes are not this unbalanced that we have to stratify the splits. At least given the amount of data that we have to stratify by the label column. XXXXX\nHere is a code snippet to do it:\nstratification_column = \"year\"\nsamples_per_stratum = 500\ndf_stratified = (\n    df\n    .groupby(stratification_column, as_index = True)\n    .apply(lambda x : x.sample(n = samples_per_stratum))\n    .reset_index()\n    # .drop([\"level_0\", \"level_1\"], axis = 1) # Some additional columns will appear, you may want to drop them\n)"
  },
  {
    "objectID": "index.html#choose-and-load-a-models",
    "href": "index.html#choose-and-load-a-models",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "Choose and load a models",
    "text": "Choose and load a models\nIn this tutorial we use the Transformers package, maintained by HuggingFace. HuggingFace is a company specialised in NLP and machine learning. They maintain many SOTA libraries such as Transformers, Datasets or Sentence Transormers. On top of maintaining the libraries, they publish posts about the latest tech, tutorials on their libraries, host a forum for debugging but also propose inference and training solutions if you don’t have the hardware necessary to do them on your own laptop.\nMore specifically, they propose a range of open source models and datasets downlable through their API. In the context of this tutorial, we will start working with the famous BERT models named: google-bert/bert-base-uncased\n\n\n\n\n\n\nHow to choose a model\n\n\n\nModels and their performance is highly sensible to the task and context of usage. When implementing an NLP task you might want to try several models to compare their performance. When choosing a model you want to take into account the following parametes:\n\nWhat task was the model trained for? You can see the list of tasks when browsing HuggingFace’s models. In the context of this tutorial, we want to perform a Text Classification task.\nWhat language was the model trained on? This one is trivial. However, depending on your corpus, you might want to find models that are multilingual. Performance of multilingual models can be below monolingual models; consider trying different models. You can also try and translate your corpus into one language and use a monolingual model.\nWhat documents was the model trained on? Evaluated on? Models performance can vary a lot when used on a certain domain or another. Make sure to use models that have been XXX BENCSSMARK\nLast but not least: How big is the model? You won’t get away with it, your laptop has limited computation capabilities, large models won’t run on a mere laptop. Know your hardware and don’t be greedy\n\n\n\nTo load the model we just need to use one of the AutoModel instance. Auto Classes are ready-to-use classes that will set up the right architecture depending on your task. In our case we are going to use the AutoModelForSequenceClassification. To load the model we use the following command:\nfrom transformers import AutoModelForSequenceClassification\n\nlabels = list(df[\"label_text\"].unique())\nnum_labels = len(labels)\nid2label = {id:label for id, label in enumerate(labels)}\nlabel2id = {label:id  for id, label in enumerate(labels)}\n\nMODEL_NAME = \"google-bert/bert-base-uncased\"\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, \n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id, \n    device=\"cpu\"                                          \n)\nNota: for some models you might need to set trust_remote_code to True. Make sure you trust the model you are downloading.\nWe can easily observe the general architecture of the model by typing:\nprint(model)\n\n\n\n\n\n\nModel description\n\n\n\nBertForSequenceClassification(\n  (bert): BertModel(\n    \n    &gt;&gt;&gt; This is the first step of the encoding process, use ready made embeddings \n    that will be modified with attention. We can find useful information: \n    - 30522 is the vocab size, there are 30522 entities for which there is a ready \n    made embedding.\n    - 768 is the dimension of the output embeddings\n\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n\n    &gt;&gt;&gt; This is where the attention takes place. You can see that the encoder is a\n    stack of 12 BERTlayers, themselves containing a self attention module. We find \n    again the dimension of the output embeddings: 768\n\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n\n    &gt;&gt;&gt; This pooler is the component that will take the embedding of each token in \n    the sequence and produce a unique embedding, supposedly best representing the sequene.\n\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n\n  &gt;&gt;&gt; This last component is the classifier layer, a neural network composed of 768 \n  + 3 nodes. The out_features=3 corresponds to the number of label we are using for \n  annotation! \n\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)\n\n\n\n\n\n\n\n\nTips and tricks regarding the model\n\n\n\nThe AutoConfig class is very convenient for retrieveing information that may difficult to get otherwise. We load it like this:\nfrom transformers import AutoConfig\nprint(AutoConfig.from_pretrained(MODEL_NAME))\nBertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \n  &gt;&gt;&gt; Embedding dimension \n  \"hidden_size\": 768,\n  \n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \n  &gt;&gt;&gt; Maximum context window\n  \"max_position_embeddings\": 512,\n  \n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.52.4\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n\n  &gt;&gt;&gt; Vocab size identified earlier\n  \"vocab_size\": 30522\n}\nIf you ever wanted to work with another model that has already been fine-tuned and wanted to only train the classification layer, there are two solutions:\n\nYou either generate the embeddings, save them, and train a scikit-learn classifier on top of it.\nYou use Setfit"
  },
  {
    "objectID": "index.html#train-the-model",
    "href": "index.html#train-the-model",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "Train the model",
    "text": "Train the model\nWe are finally going to train the model. To do so, we are going to follow the 3 folowing steps:\n\nTokenize your dataset\nSetup the training arguments\nLaunch training\n\n\nTokenize the dataset:\n\n\n\n\n\n\nWhat are tokens already?\n\n\n\nXXXX\n\n\nThe tokenizer needs to be loaded just as we did with the model:\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nThen choosing the parameters for the tokenizer and writing a preprocessing function:\nfrom datasets import DatasetDict, Dataset\n\n# Create a dataset from the splits we created before\ngrouped_ds_split = df_split.groupby(\"split\")\ndsd = DatasetDict({\n    split : Dataset.from_pandas(grouped_ds_split.get_group(split))\n    for split in [\"train\", \"train_eval\", \"test\", \"final_test\"]\n})\n\ntokenizer_parameters = {\n    \"truncation\":True, \n    \"padding\":\"max_length\",\n    \"max_length\":400,\n    \"return_tensors\":\"pt\"\n}\n\ndef preprocess_dataset(row: dict):\n    tokenized_entry = tokenizer(row[\"sentence-preprocessed\"], **tokenizer_parameters)\n    return {\n        **row.copy(),\n        \"labels\": int(label2id[row[\"label_text\"]]),\n        \"attention_mask\" : tokenized_entry[\"attention_mask\"].reshape(-1).to(device = \"mps\"),\n        \"input_ids\" : tokenized_entry[\"input_ids\"].reshape(-1).to(device=\"mps\")\n    }\n\n\ndsd = dsd.map(preprocess_dataset, batch_size=32)\ndsd\nThe data is ready to be used for Training!\n\n\nUnderstand the ins and outs of the pipeline\n\nentry = [\n    \"Hello World\",\n    \"This is a second query\"\n]\n\ntokenizer_parameters = {\n    \"truncation\":True, \n    \"padding\":\"max_length\",\n    \"max_length\":400,\n    \"return_tensors\":\"pt\"\n}\n\nmodel_input = tokenizer(entry,**tokenizer_parameters)\nbase_model_output = classif_model.base_model(**model_input)\nclassif_model_output = classif_model(**model_input)\nprint(f'''\n# model input keys: {', '.join(model_input)}\nmodel input shape (pytorch tensor): {model_input[\"input_ids\"].shape}\nbase model output keys: {', '.join(base_model_output)}\nbase model output last_hidden_state shape (pytorch tensor): {base_model_output.last_hidden_state.shape}\nclassification model output key: {', '.join(classif_model_output)}\nclassification model output logits shape (pytorch tensor): {classif_model_output.logits.shape}\n''')\n\n\nSetup Training parameters\nThe training parameters are store in the TrainingArguments object. We have selected the main parameters that you’ll want to look for during training but you can browse the 117 parameters it can take. You’ll find more information about tuning the hyperparameters at the Section 4.1.\nfrom transformers import TrainingArguments, Trainer, DataCollatorWithPadding\ntraining_arguments = TrainingArguments(\n    # Hyperparameters\n    num_train_epochs = 5,\n    learning_rate = 5e-5,\n    weight_decay  = 0.0,\n    warmup_ratio  = 0.0,\n    optim = \"adamw_torch_fused\",\n    # Second order hyperparameters\n    per_device_train_batch_size = 4,\n    per_device_eval_batch_size = 4,\n    gradient_accumulation_steps = 8,\n    # Metrics\n    # metric_for_best_model=\"f1_macro\",\n    # Pipe\n    output_dir = \"./models/training\",\n    overwrite_output_dir=True,\n    eval_strategy = \"epoch\",\n    logging_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    load_best_model_at_end = True,\n    save_total_limit = 5 + 1,\n\n    disable_tqdm = False,\n)\n\n\nLaunch training\nFinally, we can start the training.\ntrainer = Trainer(\n    model = classif_model, \n    args = training_arguments,\n    train_dataset=dsd[\"train\"],\n    eval_dataset=dsd[\"train_eval\"],\n)\n\ntrainer.train()\n\n\nAs a result you will get get a list of folders (depending on the “save_strategy” you set), each containing the following documents:\n\nconfig.json: a dictionnary with the config, similar to AutoConfig.\nmodel.safetensors: a file containing all the weights of the Model.\noptimizer.pt, rng_state.pth and scheduler.pt: a copy of the optimizer, rng_state and scheduler used during training.\ntrainer_state.json: a dictionnary containing all metadata up to the current checkpoint.\ntraining_args.bin: a copy of the training arguments.\n\nThe model can be loaded like before:\nreload_model = AutoModelForSequenceClassification.from_pretrained(\n    \"./models/training/checkpoint-4/\")"
  },
  {
    "objectID": "index.html#predict-the-labels-for-the-full-dataset",
    "href": "index.html#predict-the-labels-for-the-full-dataset",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "Predict the labels for the full dataset",
    "text": "Predict the labels for the full dataset\nTo predict the labels, we just need to use the model’s __call__ method and retrieve the predicted logit. From there, using the np.argmax function, we retrieve the predicted label and save the results to avoid re-running the inference.\nlabels_true : list[int] = []\nlabels_pred : list[int] = []\n\nfor batch in dsd[\"test\"].batch(batch_size=16, drop_last_batch=False):\n    model_input = {\n        'input_ids' : batch['input_ids'],\n        'attention_mask' : batch['attention_mask']\n    }\n\n    logits : np.ndarray = model(**model_input).logits.detach().numpy()\n    \n    batch_of_true_label = [np.argmax(row).item() for row in batch[\"labels\"]]\n    labels_true.extend(batch_of_true_label)\n\n    batch_of_pred_label = [np.argmax(row).item() for row in logits]\n    labels_pred.extend(batch_of_pred_label)\n\n(\n    pd.DataFrame({\n        \"predict\" : labels_pred, \n        \"gold_standard\": labels_true\n    })\n    .to_csv(\"./outputs/prediction\", index = False)\n)"
  },
  {
    "objectID": "index.html#evaluate-performance",
    "href": "index.html#evaluate-performance",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "Evaluate performance",
    "text": "Evaluate performance\nTo evaluate classification, we have many options. We list the main metrics use to evaluate the metrics performance :\n\n\n\nPrecision : Proportion of predicted elements of a certain class are correct. Maximize if you want\n\n\\[prec = \\frac{\\Sigma 1_{\\hat y = c} 1_{y = c}}{ \\Sigma 1_{\\hat y = c}}\\]\n\nRecall: Proportion of a certain class correctly predicted. Maximize if you want to create a filter, i.e. maximizes the number of elements retrieved.\n\n\\[recall = \\frac{\\Sigma 1_{\\hat y = c} 1_{y = c}}{ \\Sigma 1_{y = c}}\\]\n\nAccuracy: Proportion of all elements correctly predicted. Maximize if…\n\n\\[\\Sigma 1_{\\hat y == y}/ N\\]\n\nF1-score for one class:\n\n\\[F1(c) = 2 \\times \\frac{prec(c) recall(c)}{prec(c) + recall(c)}\\]\n\nF1-score macro: mean of all F1-score accross classes\n\n\\[ F1_{macro} = \\sum_{c} F1(C)\\]\n\nF1-score micro: F1 score weighted by the number of elements per class.\n\n\\[F1_{micro} = \\sum_c \\frac{n_c}{N}F1(C)\\]\n\n\n\n\n\n\n\nFigure 4: precision and recall\n\n\n\n\n\nIn general, we recommand using the macro F1 as it is the most representative of the model performance and works well with imbalanced datasets. Either way, you can use scikit-learn to compute all metrics. The classification_report summarises all the metrics presented above:\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_true = labels_true, y_pred = labels_pred))\n              precision    recall  f1-score   support\n\n      agrees       0.70      0.75      0.72        75\n   disagrees       0.56      0.49      0.52        45\n     neutral       0.73      0.74      0.73        84\n\n    accuracy                           0.69       204\n   macro avg       0.66      0.66      0.66       204\nweighted avg       0.68      0.69      0.68       204\nHere we can see that the model is sub optimal to say the least. The model"
  },
  {
    "objectID": "index.html#read-the-learning-curve",
    "href": "index.html#read-the-learning-curve",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "Read the learning curve",
    "text": "Read the learning curve\nAnother key insight is the learning curve. We can retrieve it by using the logs in the trainer_state.json. Let’s retrieve it and plot it:\nimport json\nimport plotly.express as px \n\nwith open(\"./models/training/trainer_state.json\", \"r\") as file:  # MODIFY !!!\n    training_state = json.load(file)\n\nloss = []\n\nfor log in training_state [\"log_history\"]:\n    step = log[\"step\"]\n    if \"loss\" in log:\n        loss += [{\"step\": step, \"loss\": log[\"loss\"], \"split\": \"train\"}]\n    elif \"eval_loss\" in log:\n        loss += [{\"step\": step, \"loss\": log[\"eval_loss\"], \"split\": \"eval\"}]\n    else: \n        # thweird\n        print(log)\n\nloss = pd.DataFrame(loss)\n\npx.line(loss, x = \"step\", y = \"loss\", color =  \"split\") \n\n\n\n\n\n\nFigure 5: Learning curve\n\n\n\n\n\nThe learning curve follows the expected pattern. The loss curve for the train split decreases and reaches 0 over time whereas the loss curve on the train-eval split decreases until it reaches a minimum and then increases. We want the learning curve to look like that, and we want the minimum to be as low as possible.\n\nIf the loss curves do not decrease or too little, this might be a sign that the learning rate was to low. Nothing’s been learned\nIf the loss do not decrease, this might be a sign that the learning rate is too high. The model is un-learning.\nIf the loss curve on the training split decreases but not the loss curve on the train-eval split, this means that the model cannot generalise well. In that case, consider increasing the weight decay.\n\n\n\n\n\n\n\n\n\nFigure 6: Typical learning curve"
  },
  {
    "objectID": "index.html#sec-hyperparameters-tuning",
    "href": "index.html#sec-hyperparameters-tuning",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "Hyperparameters tuning",
    "text": "Hyperparameters tuning\nIn the Section 3.4.3 we used the default values for the hyperparameters. In this section, we will describe them and provide advices for tuning them.\n\n\n\n\n\n\n\n\n\n\n\nDefinition\nReference\nAdvice on how to tune it\nImportance\n\n\n\n\nLearning rate\nThis is the initial “push” when training your model. Each step (??) this value is updated, however, this initial value may lead to different results.\n\\([1e-6,1e-4]\\)\nConsider increasing it if the loss does not decrease enough and decrease it if the loss diverges\n🟢🟢🟢 \n\n\nNumber of epochs\nThis is the number of time that the model will see the full train set.\n3-5\nYou want it as low as possible but high enough to reach the optimum\n🟢\n\n\nWeight Decay\nThis parameter prevents the model from overfitting by freezing random nodes during a training step\n\\(0.1\\)\nIf you have good results on the training set the model performs poorly on the train-eval set and test set consider increasing it.\n🟢\n\n\nBatch size and gradient accumulation\nThe batch size is the number of elements seen during one step. This parameter is to be tuned according to your hardware (small hardware, lower batch size). However, to get better results, we want the optimisation step to be considering many step at once, this is why we have the gradient accumulation step. This allows to execute the gradient descent with virtually more elements than the batch size\nMaintain batch_size * gradient_accumulation_steps = 32 or 64\n🔴\n\n\n\nWarmup ratio\nXXX\nXXXX\n0.1\nXXX\n\n\nOptimizer\nThis is the algorithm chosen to perform the optimisation. The most famous is SGD (Stochastic Gradient Descent) but many alternatives exist. For finetuning, don’t bother digging into it and stick with the AdamW which is made for easy tuning of the learning rate and weight decay\nAdamW\n🔴🔴🔴"
  },
  {
    "objectID": "index.html#use-gpu-for-faster-training",
    "href": "index.html#use-gpu-for-faster-training",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "Use GPU for faster training",
    "text": "Use GPU for faster training\nFor now we have only used the CPU to perform the training, but it is slow and alternatives exist. If you have a NVIDIA GPU3 and have set up your cuda4 environment, you may accelerate the process. Also, if you have a Mac with an MX chip, you can use mps to accelerate the training.\nYou can check if your environment is ready to use cuda and mps with the following lines:\nfrom torch.cuda import is_available as cuda_available\nfrom torch.mps import is_available as mps_available\n\nprint(\"CUDA: \", cuda_available())\nprint(\"MPS: \", mps_available())\nFrom there, you will need to be aware of where your objects are. Indeed, in order to use the model on CUDA (or MPS), the input and the model need to be stored on the same hardware. The code becomes:\nfrom torch import Tensor \n\nDEVICE = \"cuda\" # or \"mps\", or \"cpu\"\n\nentry = [\n    \"Hello World\",\n    \"This is a second query\"\n]\n\ntokenizer_parameters = {\n    \"truncation\":True, \n    \"padding\":\"max_length\",\n    \"max_length\":400,\n    \"return_tensors\":\"pt\"\n}\n\n\nmodel = model.to(device = DEVICE)\n\nmodel_input = tokenizer(entry,**tokenizer_parameters)\nmodel_input = {\n  \"input_ids\" = Tensor(model_input[\"input_ids\"]).to(device = DEVICE),\n  \"attention_mask\" = Tensor(model_input[\"attention_mask\"]).to(device = DEVICE)\n}\nbase_model_output = model.base_model(**model_input)\nclassif_model_output = model(**model_input)\nIf you don’t move the elements properly, you will face this error:\nExpected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\nWhen using a Dataset or DatasetDict, you can force all arrays to be automatically stored on a same device using this command:\nds = ds.with_format(\"torch\", device = DEVICE)\n\n\n\n\n\n\nTraining arguments to chose the device to use\n\n\n\nThe Training arguments object accepts the following arguments use_cpu, use_mps, use_cuda which supposedly handles everything for you. However, we would advise to manually move your objects on the right device and set:\ntraining_args = TrainingArguments(\n  ...\n  use_mps   = DEVICE == \"mps\",\n  use_cuda  = DEVICE == \"cuda\",\n  use_cpu   = DEVICE == \"cpu\",\n)\n\n\nThe final step is to clean your GPU after using it. Otherwise you risk clogging the memory and flushing it becomes a hassle. Best practices include working inside a try/except/finally loop to make sure the memory is clean after use.\nfrom gc import collect as gc_collect\nfrom torch.cuda import empty_cache, synchronize, ipc_collect\nfrom torch.cuda import is_available as cuda_available\n\ndef clean():\n  \"\"\"Flush GPU memory\"\"\"\n  empty_cache()\n  if cuda_available():\n      synchronize()\n      ipc_collect()\n  gc_collect()\n  print(\"Memory flushed\")\n\ntokenizer, model, dsd, trainer = (None, ) * 4 # All objects that are moved to the GPU\ntry: \n  tokenize = ...\n  model = ...\n  model = ...\n\nexcept Exception as e:\n    print(\"# ERROR\" + \"#\" * 93)\n    print(e)\n    print\"#\" * 100)\n\nfinally:\n    del tokenizer, model, dsd, trainer # All objects that are moved to the GPU\n    clean_memory()"
  },
  {
    "objectID": "index.html#read-the-errors",
    "href": "index.html#read-the-errors",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "Read the errors",
    "text": "Read the errors\nYou will face many, many errors. Sometimes they are random and rerunning the script / restarting the kernel will solves the issue. Others are subtle and difficult to identify. In such case, copy pase the error in google (genAI is less likely to provide you the right answer), you’ll find forums with people who faced the same issue. As stressed in the introduction, downgrading/upgrading some libraries is likely to solve the issue.\nMany errors are raised due to the functions assuming that your columns are well named and that the data has the right format. Make sure to have the following:\n\n“text” column with the text as string\n“labels” column with the labels as int; if you have multiple labels. Make sure this is a Tensor of integers\n“input_ids” a column with the tokens truncated and padded to the same length. Make sure this is a Tensor of integers\n“attention_mask” a column with the tokens truncated and padded to the same length. Make sure this is a Tensor of integers\n\nAlso, depending on the model you’re using these columns can have different names. Make sure to check the huggingface documentation related to the model you are using as well as forums.\nIf you face this error:\nNVML_SUCCESS == r INTERNAL ASSERT FAILED at \"/home/conda/feedstock_root/build_artifacts/libtorch_1750199048837/work/c10/cuda/CUDACachingAllocator.cpp\":1016, please report a bug to PyTorch.\nIt is likely that you have reached the memory limit of your hardware. Reducing the batch size or using a smaller model is likely to solve your issue."
  },
  {
    "objectID": "index.html#save-your-model-and-results",
    "href": "index.html#save-your-model-and-results",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "Save your model and results",
    "text": "Save your model and results\n??? necessary ???"
  },
  {
    "objectID": "index.html#on-annotating-your-dataset",
    "href": "index.html#on-annotating-your-dataset",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "On annotating your Dataset",
    "text": "On annotating your Dataset\n@ Etienne"
  },
  {
    "objectID": "index.html#whats-a-good-score",
    "href": "index.html#whats-a-good-score",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "What’s a good score?",
    "text": "What’s a good score?\n@ Etienne @ Julien"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Augmented Social Scientist: train encoder models on a classification task",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSet up your cuda environment↩︎\nyou need to make sure that the model that you are using does recognise emojis↩︎\nAMD GPUs do not have access to cuda but you can work you way out with alternative solutions like ZLuda↩︎\nWhat is cuda in 3 mins; Install cuda on your machine↩︎"
  }
]